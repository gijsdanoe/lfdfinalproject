{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_FINETUNED.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUQs4fv-MFQ"
      },
      "source": [
        "from google.colab import drive\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Embedding, LSTM\n",
        "from keras.initializers import Constant\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import *\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import Dropout\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import csv\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NOTS24_-pKe",
        "outputId": "bca46037-ce6f-468d-f6a8-d4a9456f7859"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# paths\n",
        "embeddings_file = '/content/gdrive/MyDrive/Data/embeddings/glove.6B.100d.txt'\n",
        "train_file_OD = '/content/gdrive/MyDrive/Data/trainOD.csv'\n",
        "\n",
        "train_file_ID = '/content/gdrive/MyDrive/Data/trainID.csv'\n",
        "dev_file_ID = '/content/gdrive/MyDrive/Data/devID.csv'\n",
        "test_file_ID = '/content/gdrive/MyDrive/Data/testID.csv'\n",
        "\n",
        "test_trump = '/content/gdrive/MyDrive/Data/testtrump.csv'\n",
        "test_police = '/content/gdrive/MyDrive/Data/testpolice.csv'\n",
        "test_education = '/content/gdrive/MyDrive/Data/testeducation.csv'\n",
        "test_immigration = '/content/gdrive/MyDrive/Data/testimmigration.csv'\n",
        "test_economy = '/content/gdrive/MyDrive/Data/testeconomy.csv'\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQV2Vqzx_ltG"
      },
      "source": [
        "def read_embeddings(embeddings_file):\n",
        "    '''Read in word embeddings from file and save as numpy array'''\n",
        "    with open(embeddings_file, 'r') as embeddings:\n",
        "      embeddingsdict = dict()\n",
        "      for line in embeddings:\n",
        "        line = line.split()\n",
        "        word = line[0]\n",
        "        embeds = line [1:]\n",
        "        embeddingsdict[word] = np.array(embeds)\n",
        "\n",
        "      return embeddingsdict"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_BtZ_kPG5zp"
      },
      "source": [
        "def read_corpus(corpus_file):\n",
        "    '''Read in csv data set and returns docs and labels'''\n",
        "    documents = []\n",
        "    labels = []\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        lines = csv.reader(f, delimiter=',')\n",
        "        for line in lines:\n",
        "            line[0] = line[0].replace('timescontent.com', '').replace('MATP', '').replace('Reprint', '').replace('â€', '' ).replace('â€¢', '').replace('Â', '').replace('™️', '').replace('Herald', '')\n",
        "            documents.append(line[0])\n",
        "            labels.append(line[-1])\n",
        "    return documents, labels"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxJi3brNRj05"
      },
      "source": [
        "def get_emb_matrix(voc, emb):\n",
        "    '''Get embedding matrix given vocab and the embeddings'''\n",
        "    num_tokens = len(voc) + 2\n",
        "    word_index = dict(zip(voc, range(len(voc))))\n",
        "    # Bit hacky, get embedding dimension from the word \"the\"\n",
        "    embedding_dim = len(emb[\"the\"])\n",
        "    # Prepare embedding matrix to the correct size\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = emb.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    # Final matrix with pretrained embeddings that we can feed to embedding layer\n",
        "    return embedding_matrix"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvQZXgQVTs7o"
      },
      "source": [
        "def create_model(Y_train, emb_matrix):\n",
        "    '''Create the Keras model to use'''\n",
        "    # Define settings, you might want to create cmd line args for them\n",
        "    learning_rate = 0.01\n",
        "    loss_function = 'binary_crossentropy'\n",
        "    optim = SGD(learning_rate=learning_rate)\n",
        "    # Take embedding dim and size from emb_matrix\n",
        "    embedding_dim = len(emb_matrix[0])\n",
        "    num_tokens = len(emb_matrix)\n",
        "    num_labels = len(Y_train[0])\n",
        "    # Now build the model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(emb_matrix),trainable=False))\n",
        "    # Here you should add LSTM layers (and potentially dropout)\n",
        "   # model.add(LSTM(units=num_labels))\n",
        "    model.add(LSTM(units=64))\n",
        "    #Dropout    # Ultimately, end with dense layer with softmax\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile model using our settings, check for accuracy\n",
        "    model.compile(loss=loss_function, optimizer=\"adam\", metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqVQQ4a9T6IS"
      },
      "source": [
        "def train_model(model, X_train, Y_train, X_dev, Y_dev):\n",
        "    '''Trains the model'''\n",
        "    # Potentially change these to cmd line args again\n",
        "    # And yes, don't be afraid to experiment!\n",
        "    verbose = 1\n",
        "    batch_size = 16\n",
        "    epochs = 50\n",
        "    # Early stopping: stop training when there are three consecutive epochs without improving\n",
        "    # It's also possible to monitor the training loss with monitor=\"loss\"\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    # Finally fit the model to our data\n",
        "    model.fit(X_train, Y_train, verbose=verbose, epochs=epochs, batch_size=batch_size, validation_data=(X_dev, Y_dev))\n",
        "    # Print final accuracy for the model (clearer overview)\n",
        "    return model"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIhJrhRPffo0"
      },
      "source": [
        "def test_set_predict(model, X_test, Y_test, ident, labels):\n",
        "    '''Do predictions and measure accuracy on test set)'''\n",
        "    # Get predictions using the trained model\n",
        "    Y_pred = model.predict(X_test)\n",
        "    class_one = Y_pred > 0.5\n",
        "    digits = 3\n",
        "\n",
        "    ac = accuracy_score(Y_test, class_one)\n",
        "    pr = precision_score(Y_test, class_one, average='macro', zero_division=0)\n",
        "    re = recall_score(Y_test, class_one, average='macro', zero_division=0)\n",
        "    f1 = f1_score(Y_test, class_one, average='macro', zero_division=0)\n",
        "    \n",
        "    msg = f'''\n",
        "    {classification_report(Y_test, class_one, digits=digits, zero_division=0, target_names=labels)}\n",
        "    Accuracy:   {round(ac, digits)}\n",
        "    Precision:  {round(pr, digits)}\n",
        "    Recall:     {round(re, digits)}\n",
        "    F-score:    {round(f1, digits)}\n",
        "    '''\n",
        "    return msg"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypm6yAIr_94P"
      },
      "source": [
        "embeddings = read_embeddings(embeddings_file)\n",
        "\n",
        "# change test_file_ID to desired testset, for example read_corpus(test_immigration) for immigration\n",
        "X_train, Y_train = read_corpus(train_file_OD)\n",
        "X_dev, Y_dev = read_corpus(dev_file_ID)\n",
        "X_test, Y_test = read_corpus(test_file_ID)\n",
        "\n",
        "\n",
        "vectorizer = TextVectorization(standardize=\"lower_and_strip_punctuation\", output_sequence_length=1000)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train + X_dev + X_test)\n",
        "vectorizer.adapt(text_ds)\n",
        "voc = vectorizer.get_vocabulary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqHn6y6MQrcP"
      },
      "source": [
        "emb_matrix = get_emb_matrix(voc, embeddings)\n",
        "encoder = LabelBinarizer()\n",
        "Y_train_bin = encoder.fit_transform(Y_train)\n",
        "Y_dev_bin = encoder.fit_transform(Y_dev)\n",
        "X_train_vect = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_dev_vect = vectorizer(np.array([[s] for s in X_dev])).numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ackl3X5_X1U5"
      },
      "source": [
        "model = create_model(Y_train, emb_matrix)\n",
        "model = train_model(model, X_train_vect, Y_train_bin, X_dev_vect, Y_dev_bin)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbRZr6fQfVqU"
      },
      "source": [
        "Y_test_bin = encoder.fit_transform(Y_test)\n",
        "X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()\n",
        "\n",
        "scores = model.evaluate(X_test_vect, Y_test_bin, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "results = test_set_predict(model, X_test_vect, Y_test_bin, \"test\", labels=encoder.classes_)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze9phmcvBfiR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}